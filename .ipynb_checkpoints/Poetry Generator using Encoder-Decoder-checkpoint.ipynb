{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce75b7a",
   "metadata": {},
   "source": [
    "The aim of this project is to explore the process of generating text utilizing a character-based RNN(CREDITS GO TO GOOGLE CLOUD FOR THE PROJECT). The dataset I will be utilizing here is a dataset containing Shakespeare's writings sourced from Andrej Karpathy's \"The Unreasonable Effectiveness of Recurrent Neural Networks\". The task involves training a model to predict the succeeding character in a given sequence of characters extracted from this data (\"Shakespear\" in this case), aiming to predict the next character (\"e\"). Subsequently, by repeatedly invoking the model, longer sequences of text can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731715b",
   "metadata": {},
   "source": [
    "Environemnt Setup:\n",
    "Importing TensorFlow and any other necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4840013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set TensorFlow logging level to suppress unnecessary messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18a30e",
   "metadata": {},
   "source": [
    "Downloading the Shakespeare dataset.\n",
    "Modify the subsequent line to execute this code with any custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd4102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff44f2",
   "metadata": {},
   "source": [
    "Read the data\n",
    "Initially, we will download the file and subsequently decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7299ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the text is: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Open the file and read its contents as bytes, then decode using UTF-8 encoding\n",
    "with open(path_to_file, \"rb\") as file:\n",
    "    text = file.read().decode(encoding=\"utf-8\")\n",
    "\n",
    "# Print the length of the text to verify the number of characters\n",
    "print(f\"The length of the text is: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25655a0c",
   "metadata": {},
   "source": [
    "Let's examine the initial 250 characters within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc7d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 250 characters of the text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855a548",
   "metadata": {},
   "source": [
    "Let's determine the total count of unique characters present in our corpus or document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d21107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted set to obtain unique characters, then print the count\n",
    "vocab = sorted(set(text))\n",
    "print(f\"There are {len(vocab)} unique characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109c82e",
   "metadata": {},
   "source": [
    "Process the text\n",
    "Convert the text into numerical vectors\n",
    "Prior to training, it's essential to transform the strings into numerical forms.\n",
    "\n",
    "By employing the tf.keras.layers.StringLookup layer, each character can be converted into a unique numeric identifier. Initially, the text needs to be segmented into tokens for this process to be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf0bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = [\"abcdefg\", \"xyz\"]\n",
    "\n",
    "# Split the texts into characters\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088b8af",
   "metadata": {},
   "source": [
    "Next, instantiate the tf.keras.layers.StringLookup layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69248f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf.keras.layers.StringLookup layer\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700824df",
   "metadata": {},
   "source": [
    "It facilitates the conversion from tokens to character IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a17b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilize the created StringLookup layer to convert tokens into character IDs\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5775e17",
   "metadata": {},
   "source": [
    "As the objective of this project revolves around text generation, it's crucial to reverse this encoding process and reconstruct human-readable strings from it. To accomplish this, I am going to employ tf.keras.layers.StringLookup(..., invert=True).\n",
    "\n",
    "It is to be kept in mind that instead of providing the original vocabulary created with sorted(set(text)), the get_vocabulary() method of the tf.keras.layers.StringLookup layer is to be utilized. This ensures that the [UNK] token is handled consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d30db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StringLookup layer to convert character IDs back to tokens\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab4e45",
   "metadata": {},
   "source": [
    "This layer retrieves characters from the ID vectors and presents them as a tf.RaggedTensor composed of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba50c6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilize the StringLookup layer to convert character IDs back into tokens\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bd1a2",
   "metadata": {},
   "source": [
    "I aim to use tf.strings.reduce_join to concatenate the characters back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "095be56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilize tf.strings.reduce_join to concatenate characters into strings\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43db98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reconstruct text from character IDs\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac83d09",
   "metadata": {},
   "source": [
    "Prediction Task:\n",
    "The goal is to predict the most probable next character given a single character or a sequence of characters. This forms the basis of the model's training. The model receives a sequence of characters as input and is trained to predict the subsequent character at each time step.\n",
    "\n",
    "As recurrent neural networks (RNNs) maintain an internal state dependent on previously observed elements, they can infer the next character based on all characters seen up to that point.\n",
    "\n",
    "Creating Training Examples and Targets:\n",
    "The next step involves dividing the text into example sequences. Each input sequence will comprise seq_length characters from the text. Correspondingly, the targets for each input sequence will contain text of the same length, but shifted by one character to the right.\n",
    "\n",
    "To accomplish this, the text is segmented into chunks of size seq_length+1. For instance, if seq_length is 4 and the text is \"Hello\", the input sequence would be \"Hell\", and the target sequence would be \"ello\".\n",
    "\n",
    "Initially, employ the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddad0e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the entire text into character IDs\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec8f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the character IDs\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "# Display the first 10 characters from the dataset\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f401904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length and calculate the number of examples per epoch\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1007ee",
   "metadata": {},
   "source": [
    "The batch method simplifies the process of converting these individual characters into sequences of the specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eadc4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of the desired length using the batch method\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# Display the characters from the first sequence\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90838733",
   "metadata": {},
   "source": [
    "It becomes clearer to understand what this process entails when you concatenate the tokens back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0531af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# Display the text reconstructed from the first 5 sequences\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc6cc7",
   "metadata": {},
   "source": [
    "During training, we need a dataset consisting of pairs of (input, label), where both input and label represent sequences. At each time step, the input corresponds to the current character, while the label corresponds to the subsequent character.\n",
    "\n",
    "Below is a function that accepts a sequence as input, duplicates it, and shifts it to ensure alignment between the input and label for each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27da4e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# Example usage of the function\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ecd792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset of (input, label) pairs using the split_input_target function\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Display an example input and target pair from the dataset\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26dc39",
   "metadata": {},
   "source": [
    "Prepare Training Batches\n",
    "After segmenting the text into manageable sequences using tf.data, the next step is to shuffle the data and organize it into batches before feeding it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c7eb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define buffer size for shuffling the dataset\n",
    "# (TensorFlow's data pipeline is designed to handle potentially infinite sequences,\n",
    "# so it shuffles elements within a buffer rather than shuffling the entire sequence in memory)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle, batch, and prefetch the dataset for optimal performance\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd97ec",
   "metadata": {},
   "source": [
    "Constructing the Model\n",
    "This section outlines the creation of the model as a subclass of keras.Model (For more details, refer to Creating New Layers and Models through subclassing).\n",
    "\n",
    "Developing a model with the following layers:\n",
    "\n",
    "tf.keras.layers.Embedding: Serves as the input layer, constituting a trainable lookup table that maps each character-ID to a vector with embedding_dim dimensions.\n",
    "tf.keras.layers.GRU: An RNN type with size units=rnn_units. (Alternatively, you can opt for using an LSTM layer here.)\n",
    "tf.keras.layers.Dense: Represents the output layer, featuring vocab_size outputs. It generates one logit for each character within the vocabulary. These logits represent the log-likelihood of each character as per the model's estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64fcbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the length of the vocabulary in characters\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Set the embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Specify the number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e979f",
   "metadata": {},
   "source": [
    "The following class accomplishes the following tasks:\n",
    "\n",
    "    1.It inherits from tf.keras.Model.\n",
    "    2.The constructor is employed to specify the layers of the model.\n",
    "    3.The forward pass is defined using the layers outlined in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec2bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        # Define an embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Create a GRU layer\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units, return_sequences=True, return_state=True\n",
    "        )\n",
    "        # Connect with a dense layer\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        \n",
    "        # GRU layer\n",
    "        # During training for text generation, utilize the previous state.\n",
    "        # If no state is available, initialize it.\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(\n",
    "    # Ensure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731e05e",
   "metadata": {},
   "source": [
    "For every character, the model retrieves the embedding, executes one time step of the GRU with the embedding as input, and applies the dense layer to produce logits that predict the log-likelihood of the succeeding character.\n",
    "\n",
    "Experiment with the Model\n",
    "Next, execute the model to verify that it performs as intended.\n",
    "\n",
    "Begin by examining the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5288cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Fetch an input example batch and a target example batch from the dataset\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    # Generate predictions for the example batch\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    \n",
    "    # Display the shape of the predictions\n",
    "    print(\n",
    "        example_batch_predictions.shape,\n",
    "        \"# (batch_size, sequence_length, vocab_size)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df3d71",
   "metadata": {},
   "source": [
    "In the preceding example, although the input sequence length is set to 100, the model is capable of processing inputs of variable lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44eb5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4ec4e",
   "metadata": {},
   "source": [
    "To obtain real predictions from the model, wq must sample from the output distribution to acquire actual character indices. This distribution is determined by the logits across the character vocabulary.\n",
    "\n",
    "Note: Sampling from this distribution is crucial, as relying solely on the argmax of the distribution can potentially cause the model to become trapped in a loop.\n",
    "\n",
    "Testing this approach with the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a617d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 55, 28, 29, 61, 11, 55, 26, 27, 47, 49, 25, 59, 43, 59, 51, 12,\n",
       "       27,  9, 62, 11,  3,  6, 41, 31, 30, 44, 48, 22, 44, 63, 32, 22, 60,\n",
       "       23, 17, 44, 12, 33,  7, 20, 21, 55, 23, 14, 60, 23,  3, 45, 57, 41,\n",
       "       42, 60,  0, 55, 51, 20, 13, 64, 51, 27, 56,  0, 31, 16, 24, 55,  3,\n",
       "       12, 12, 21, 19, 21, 50, 45, 54, 31, 28, 17, 37, 60,  2, 59,  7, 58,\n",
       "       37, 50, 65, 27, 64, 12, 43, 42, 56, 52, 34, 53, 26, 40, 21],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample character indices from the output distribution\n",
    "sampled_indices = tf.random.categorical(\n",
    "    example_batch_predictions[0], num_samples=1\n",
    ")\n",
    "\n",
    "# Squeeze the sampled indices to remove unnecessary dimensions and convert to numpy array\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4cee0",
   "metadata": {},
   "source": [
    "This provides us with a prediction of the next character index at each timestep:\n",
    "\n",
    "Decoding these indices to observe the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb773c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' justice of your dealing?\\n\\nProvost:\\nBut what likelihood is in that?\\n\\nDUKE VINCENTIO:\\nNot a resemblan'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"KpOPv:pMNhjLtdtl;N.w:!'bRQeiIexSIuJDe;T,GHpJAuJ!frbcu[UNK]plG?ylNq[UNK]RCKp!;;HFHkfoRODXu t,sXkzNy;dcqmUnMaH\"\n"
     ]
    }
   ],
   "source": [
    "# Display the input text\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "\n",
    "# Display the predicted next characters\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e52d00",
   "metadata": {},
   "source": [
    "Proceed with training the model.\n",
    "At this juncture, the problem can be approached as a conventional classification task. Given the previous RNN state and the input for this time step, predict the class of the subsequent character.\n",
    "\n",
    "Include an optimizer and a loss function.\n",
    "The standard tf.keras.losses.sparse_categorical_crossentropy loss function is suitable for this scenario as it operates across the last dimension of the predictions.\n",
    "\n",
    "Since the model produces logits, it's necessary to set the from_logits flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b29eaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1887097, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Calculate the mean loss for the example batch\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "# Display prediction shape and mean loss\n",
    "print(\n",
    "    \"Prediction shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \" # (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3865da",
   "metadata": {},
   "source": [
    "In the initial stages, a freshly initialized model shouldn't exhibit excessive confidence; hence, the output logits should possess comparable magnitudes. To verify this, you can assess whether the exponential of the mean loss is roughly equivalent to the size of the vocabulary. If the loss is significantly higher, it suggests that the model is overly confident in its incorrect predictions, indicating poor initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c85527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.93766"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the exponential of the mean loss and convert to numpy array\n",
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663e93",
   "metadata": {},
   "source": [
    "Set up the training process by utilizing the tf.keras.Model.compile method. Employ tf.keras.optimizers.Adam with default parameters along with the specified loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79181983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training using the Adam optimizer and the specified loss function\n",
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f11e41",
   "metadata": {},
   "source": [
    "Set up checkpoints\n",
    "Utilize tf.keras.callbacks.ModelCheckpoint to guarantee that checkpoints are saved throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6430cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory for saving checkpoints\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# Define the prefix for checkpoint filenames\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Create a callback to save model weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6500ee",
   "metadata": {},
   "source": [
    "Proceed with the training process.\n",
    "To maintain reasonable training duration, conduct training over 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11754d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 256s 1s/step - loss: 2.7131\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 354s 2s/step - loss: 1.9851\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 354s 2s/step - loss: 1.7029\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 353s 2s/step - loss: 1.5431\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 353s 2s/step - loss: 1.4458\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 361s 2s/step - loss: 1.3779\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 350s 2s/step - loss: 1.3249\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 355s 2s/step - loss: 1.2812\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 353s 2s/step - loss: 1.2399\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 352s 2s/step - loss: 1.2006\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "# Train the model over the specified number of epochs, utilizing the checkpoint callback\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9d081",
   "metadata": {},
   "source": [
    "Text Generation\n",
    "One straightforward method to generate text using this model is to iterate through it in a loop while monitoring the model's internal state during execution.\n",
    "\n",
    "For each model call, provide some text along with its internal state. The model will then predict the next character and provide its updated state. To continue generating text, feed the prediction and state back into the model.\n",
    "\n",
    "The following code snippet demonstrates making a single-step prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbd25bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        # Set the temperature parameter for sampling\n",
    "        self.temperature = temperature\n",
    "        # Store the model, functions for character-to-ID and ID-to-character conversion\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Set -inf at each invalid index.\n",
    "            values=[-float(\"inf\")] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape: [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "        # Consider only the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        # Apply the prediction mask to prevent \"[UNK]\" generation.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample token IDs from the output logits.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert token IDs to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states\n",
    "\n",
    "# Instantiate the OneStep model\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283055",
   "metadata": {},
   "source": [
    "Execute the model within a loop to produce some text. Upon examining the generated text, we can notice that the model demonstrates an understanding of when to capitalize, create paragraphs, and mimic a vocabulary reminiscent of Shakespeare's writing style. However, due to the limited number of training epochs, it has not yet acquired the ability to construct coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ecdbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The town of, or dow, the Volsces' haspy true\n",
      "Reisolder: but, one firely on this them.\n",
      "\n",
      "PETRUCHIO:\n",
      "Within your accuselers are all this: and you go?\n",
      "\n",
      "ROMEO:\n",
      "We'll to be found, give me, and I hade, for their\n",
      "intercaited:\n",
      "Fare you well mouth by me?' touch my prettience\n",
      "to him to make it the assisting on their curse to any.\n",
      "\n",
      "YORK:\n",
      "Trayon with his great surses, brave Gruicent:\n",
      "If I protest, was brave men's, and myself and be me\n",
      "But little like an old than thy thing achieved, so\n",
      "\n",
      "JUFIE:\n",
      "Where is young cick, word of her king?\n",
      "\n",
      "ESTALUS:\n",
      "Ay, widow'll gold it, if you be revenged?\n",
      "If they do the time leave onserve:\n",
      "Add, pray, so would Marcius borne so heart's\n",
      "That when I wishey persuade,--\n",
      "Since being none but determined on thee.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Of these he has me near out of mine arch you of it!\n",
      "\n",
      "MARIANA:\n",
      "'Tis receive's will take to make hast thus but no hand.\n",
      "\n",
      "KING RICHARD III:\n",
      "Ready than thou hast wound 'em; shall this one according,\n",
      "That banish'd his friends, shall she is buried\n",
      "Binguish'd b \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.5928175449371338\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "# Generate text in a loop\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "# Join the generated text\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8500f0",
   "metadata": {},
   "source": [
    "To enhance the results, consider the following options:\n",
    "\n",
    "    1.Extend the training duration by increasing the number of epochs (e.g., try EPOCHS = 30).\n",
    "    2.Experiment with different start strings.\n",
    "    3.Enhance the model's accuracy by adding an additional RNN layer.\n",
    "    4.Adjust the temperature parameter to control the level of randomness in predictions.\n",
    "\n",
    "For faster text generation, batch the text generation process. The following example demonstrates generating five outputs in approximately the same time it took to generate one output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b17cb6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nNay, I do forgot the fault o' the infinity\\nProud I proce'd to Polizenes.\\n\\nHENRY SE:\\nThen let the sour of Clarence' nessore'd him,\\nSome with the holy manners of his mind:\\nOn an hour confessors are for't,\\nNo worer tham we dather with riage: fellow, sir;\\nFor do I their betites now your own fieth,\\nBut kill the worl: he long in our rebories,\\nBut like the wasting on the very sense to come from you,\\nthat let it was your nock; what, if it consul?\\n\\nBRAKENBURY:\\nAre you say 'twere at the wind; then, sound God, he'll thy fight\\nAnd hidune and the Earl of your redain,\\nBegins an Engress valiant credit to rust him;\\nFor once make hands frewly he't be so doubt it, shall I do bid\\nthin. Worthy save my poor man?\\nWe have heard of love and secret words\\nFor can carried to pluck with the villain:\\nWhich'el carnot blume my plaint\\nIn her whole wratich, who bad's made thee?\\n\\nKING RICHARD III:\\nSo long his heart; 'twomanity and his up, a\\ncounsel master men.\\n\\nSecond Senator:\\nthere's some goodly good crown;\\nThe most \"\n",
      " b\"ROMEO:\\nKith it.\\n\\nSecond Gentleman:\\nNo, my father, I'll know, the serving in the\\nCupure; that I never hang us born't\\nTo her and said indeed. Is that\\nwould be ridal house.\\n\\nSecond Gentleman:\\nWith saint she? by and look in thy throne;\\nNext for a thousand stain\\nWith two viteem unto my fait, and with some chaim\\nof this fair cloudy man art as he; or else for his\\nmast ruin: 'tis worth is injurion through his Edward's neast,\\nDo with my plot; I will barge me in great.\\nCull heart that this lature lose it is.\\n\\nRoman:\\nI willow it outs: 'tis but a mine.\\n\\nShepherd:\\nTalk not, for Warwick!\\nThis let mark than the wardaning again.\\n\\nKING RICHARD II:\\nWe have alzember of this full thy light\\nThat we plead with his temper, sought the crown.\\n\\nTYBALT:\\nWelcome, make me sent for rushes, brights\\nWith viowed sheat confolding than the\\nmost platterer; he is breathed proud friends, I\\nknew shame. You are come, two make them an oaf,\\nThat takes disgladed them, this heart, to make\\nYour finger than by of ither fuenful highway:\\n\"\n",
      " b\"ROMEO:\\nHa, had he proclaim'd and pursuil winow\\nWas the soldierlust rores: the glorious unward's\\nOf you hither.\\n\\nPROSPERO:\\nForseom the father again.\\n\\nSecond Citizen:\\nReppare, for evil; stay, cousins, false!\\n\\nRIVERS:\\nSo do so tent the proud that worth'd wither,\\nThe make for vengeance of this tame pleasued throught you?\\n\\nSIANA:\\nNo, no hand,\\nThe queen of her. What brace of mine?\\nYoure is the tubil. What canst thou didst\\nWe dispior with your give his enemies:\\nMeth such a foun return rewise his man! what art\\nWhich to Carise in his own senting wars.\\n\\nJOHN MORBARD WILLOUGHBY:\\nAnd look'd me to my fearty, and these no combassed doubt,\\nAnd make thee justy sick, nor to you, the best gestrokes:\\nAnd grief to me in. If she will make misfisch'd with old\\nThan want of holours; though the aughbellow of my officess:\\nThere way; and misthubour company:\\nAll come hither, my hear, sigh;\\nWe will obey at the steelful of orn\\nTo troops of Ken.\\nThey looking bed in your will, if it\\nwere bornd dishoney to that enter'd you \"\n",
      " b\"ROMEO:\\nAnother smiles and myself; and we owe to\\nOn your wits I scarce and dely serve,\\nFor little she is fined, doth countenance, sir;\\nFor still from their gentleman; and the little;\\nNothing but let not a too much,\\nHath marray endury in the warrant? from these\\ngaining Judious, man? that would have suitors\\nThe same blood he is as hanging:\\nThen lie in his good northion's fait;\\nSpake in resemble, insulled friend,\\nAnd he, thy trunt we deserves ready, their consider'd throats\\nOf the namposess is out. Come, cover from all,\\nTo most the drunk of heaven, and the law, or your grief,\\nTo twentiest the wive sends you for his, honourable.\\nSproudder than the time, that has the north,\\nThe light-winter of your beholdite,\\nThat remains as grievous must be benit to\\nuptiracul with less executive with great anger-out.\\n\\nThird Citizen:\\nNo, no, my order hast for parts\\nThis half in Pamin, take it out for: there is sir,\\nThat quoth that even: for one I please her, say\\nThat it shall pass your will, the thiry near,\\nCan th\"\n",
      " b\"ROMEO:\\nAdain! you have I adventure\\nWhile we profts thee from it? Thou unsweet.\\n\\nTail: Ke't beging?\\n\\nServant:\\nWidh fine is your confine in my you.\\n\\nSEBASTIAN:\\nNo, must be so contry: fit.\\n\\nSecond Citizen:\\nYou master on. Anointed bits his master.\\n\\nSheeFFRAMINA:\\nIf you by haste you?\\n\\nMERCUTIO:\\nResp you! gentlemen, content you, my lord.\\n\\nKING HENRY VI:\\nRomeo lightning means, shame the rest, as we was ginger;\\nNow I be true? That slain he process villain,\\nWast thou soese trite us and yours,\\nRichir that one wend these I dressed blind,\\nThe oving way to cry to me, and they say thee\\nbegan, or elder sail the sacrament-geering ring.\\n\\nKATHARINA:\\nAdhocent man, 'tis the lady won on.\\n\\nThurd Merand:\\nHark, I do beseech you, I did it.\\n\\nDUKE VINCENTIO:\\nThese five that serves me more.\\n\\nHeSRYONES:\\nPut not:\\nOn what will you chide it to fame as swoce and barr.\\nThat ere she woe to The Elizares not to\\ncorrupt partitually and lies, and to no furries\\nThan whom we mean to any that humble see\\nIf your high issue: I proud t\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.4238874912261963\n"
     ]
    }
   ],
   "source": [
    "# Record the start time\n",
    "start = time.time()\n",
    "\n",
    "# Initialize model states\n",
    "states = None\n",
    "\n",
    "# Define the starting string\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "# Generate text in a loop\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "# Join the generated text\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "# Record the end time\n",
    "end = time.time()\n",
    "\n",
    "# Print the generated text\n",
    "print(result, \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e381ed",
   "metadata": {},
   "source": [
    "Export the text generator\n",
    "This one-step model can be effortlessly saved and restored, enabling its use wherever a tf.saved_model is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c283f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000023789DFA040>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "But I drink not requite stretchrels disormeded\n",
      "By a piscourse in the duke with his fingerous shigh;\n"
     ]
    }
   ],
   "source": [
    "# Save the one-step model\n",
    "tf.saved_model.save(one_step_model, \"one_step\")\n",
    "\n",
    "# Load the saved model\n",
    "one_step_reloaded = tf.saved_model.load(\"one_step\")\n",
    "\n",
    "# Initialize model states\n",
    "states = None\n",
    "\n",
    "# Define the starting string\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "# Generate text in a loop\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "# Print the generated text\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
